# Loss Function (Hàm mất mát) và Objective Function (Hàm mục tiêu)

Thông thường chúng ta thường thực hiện công việc tối ưu hóa một hàm mục tiêu hay làm cực tiểu một hàm mất mát (loss function).

## Vấn đề ước lượng hàm số trong machine learning

Trong machine learning, bạn được cung cấp một tập hữu hạn các quan sát (dữ liệu) và phải từ số lượng hữu hạn dữ liệu này xây dựng được một mô hình hoặc một quy luật tổng quát cho bài toán sao cho mô hình của bạn có thể dự đoán đúng cả trong những trường hợp dữ liệu mới được đưa vào không có trong tập dữ liệu hữu hạn đã được quan sát.

Nếu bạn đã học qua chương trình Vật lý phổ thông, hẳn các bạn đều biết đến một truyền thuyết rằng Newton bị quả táo rơi vào đầu và nghĩ ra định luật vạn vật hấp dẫn, Einstein quan sát các chùm sáng và phát hiện ra thuyết tương đối. Nhưng thực sự từ những ý tưởng mơ mộng đó đến các phương trình vật lý $F = ma$ hay $E = mc_2$ là cả một quãng đường thí nghiệm và nghiên cứu trải dài hàng năm thậm chí hàng chục năm. Các nhà khoa học sẽ phải quan sát rất nhiều số liệu đo đạc từ thực tế, sau đó đưa ra các phương trình “hợp lý” cho các số liệu đo đạc đó (tức là sai số nhỏ nhất). Điều này giống hệt với những gì chúng ta đã làm trong bài trước về ví dụ xác định mức độ hài lòng với công việc theo mức lương. Tất nhiên các bài toán khoa học thì phức tạp hơn, nhiều tham số hơn và tham số đó cũng khó đo đạc hơn. Nhưng nhìn chung, mọi chuyện diễn ra khá tương tự.

![A](https://tek4.vn/public_files/fddf0d3d-bfb1-485a-b226-aca1aba94201)

Tuy nhiên, có ba vấn đề nghịch lý xảy ra trong thực tế khoa học, cũng như trong chính machine learning đó là:

- Dữ liệu trong một bài toán là vô hạn, cũng như các quan sát trong thực tế cũng là vô hạn nhưng sức người có hạn và chỉ có thể có được một phần hữu hạn (thường là rất nhỏ) trong toàn bộ số lượng vô hạn các quan sát đó. Và kể cả chúng ta có thu được một cách đầy đủ vô hạn dữ liệu thì cũng không có máy tính hoặc năng lực tính toán nào có thể xử lý nổi
- Ngoài ra, dữ liệu khi đo đạc luôn tồn tại các sai số ngẫu nhiên và không phải lúc nào cũng đúng 100%.
- Chúng ta không thực sự biết thế giới này tồn tại như thế nào, và cũng chẳng biết các mô hình chính xác trong bài toán là như thế nào nên gần như chúng ta sẽ phải đi tìm cái mà chúng ta không bao giờ biết.

Chính vì những lý do đó mà các mô hình machine learning hay kể cả những mô hình khoa học chúng ta tìm thấy và sử dụng đều là không thể xác định nó có đúng hay không nếu không được thực nghiệm đủ tốt trong thực tế. Chúng ta chỉ có mô hình đủ tốt chứ không có mô hình chính xác. Hay nói cách khác, bạn cần tìm một hàm bí ẩn $f^*$ nào đó từ một loạt các mẫu quan sát $x^{(1)}, ..., x^{(N)}$ cho trước, trong đó có thể có chứa các nhān $y^{(1)} = f^* (x^{(1)}, ..., y^{(N)} = f^* (x^{(N)}$ tương ứng. Nhưng bạn lại chẳng thể biết chính xác hàm $f^*$ là gì mà chỉ biết các dữ liệu quan sát $(x^{(i)},y^{(i)})$ với đầy nhiễu đo đạc.

Ví dụ: Chúng ta có một hàm $f^* = x^2$ và các cặp giá trị tương ứng $(1,1), (2,4), (3,9), (5,25).$ Nếu ta biết hàm số có dạng bậc 2 thì mọi chuyện cực kỳ đơn giản, chúng ta có thể dễ dàng suy ra được hàm $f^*$, nhưng vấn đề là chúng ta lại không biết dạng thực sự của hàm có phải là bậc 2 hay không. Đến đây, chắc chắn để khôi phục lại hàm là điều cực kỳ khó khăn chưa kể đến trường hợp có nhiễu trong dữ liệu quan sát.

Vì ta không thể nào biết được $f^*$ có dạng như thế nào. Do đó, dù ta có đưa ra một mô hình $f_w$ để ước lượng $f^*$, thì ta cũng không biết chính xác $f^*$ là gì để biết là ta đang đúng hay sai. Nói cách khác, machine learning là một trò chơi dự đoán mà không ai biết đáp án đúng là gì.

![A](https://tek4.vn/public_files/4849abc6-c0a7-498a-9219-f971faf981f4)

Có vẻ đến đây chúng ta nghe khá nản rồi phải không ạ? Tuy nhiên, đừng lo lắng, bạn vẫn thấy các phương trình của Newton hay Einstein “chạy” tốt chứ? Tôi thì thực sự không biết nó có chính xác hay không, nhưng cho đến giờ, người ta vẫn phải làm hàng ngàn thí nghiệm thực tế để kiểm chứng lại các lý thuyết này cho đến khi nó sai thì thôi. Mà thực ra cơ học của Newton đã được chỉ ra là không chính xác trong vật lý lượng tử, nhưng đâu có quan trọng, nó vẫn hoạt động tốt trong các hoạt động hàng ngày là được mà. Trong machine learning cũng vậy, chúng ta không cần biết nó có thực sự đúng hay không mà chúng ta chỉ cần nó đúng trong thực nghiệm. Điều này khác biệt hoàn toàn với toán học phải đúng về mặt lý thuyết, machine learning cũng như các ngành khoa học thực nghiệm khác chỉ cần quan tâm đến việc nó đúng trong thực tế. Nói cách khác, cho dù chúng ta không biết hình dạng của $f^*$ ra sao, thì chúng ta vẫn còn đó các dữ liệu sinh ra từ hàm này để có thể đánh giá độ tốt của một model trên các dữ liệu thực tế này. Và mục tiêu của chúng ta đó là tối ưu độ đo hiệu quả P trên dữ liệu thực tế sao cho lỗi dự đoán là tối thiểu trên dữ liệu thực tế. Do độ đo P tối ưu không phải bao giờ cũng dễ dàng để tối ưu (do tính rời rạc, không có đạo hàm…) do đó, chúng ta thường ánh xạ nó sang những đại lượng tương tự mà có thể biểu diễn được dưới dạng các hàm số có thể tối ưu được gọi là các hàm mục tiêu (objective function) hoặc hàm mất mát (loss function).

## Loss Function và Objective Function

Về mặt bản chất, loss function (hàm mất mát) là hàm cho phép xác định mức độ sai khác của kết quả dự đoán so với giá thực thực cần dự đoán. Nó là một phương pháp đo lường chất lượng của mô hình dự đoán trên tập dữ liệu quan sát. Nếu mô hình dự đoán sai nhiều thì giá trị của loss function sẽ càng lớn và ngược lại nếu nó dự đoán càng đúng thì giá trị của loss function sẽ càng thấp.

Ví dụ 1. Ở đây, để cho dễ hiểu, chúng ta định nghĩa một hàm loss function đơn giản như sau. Xét bài toán hồi quy mức độ hài lòng với công việc theo mức lương ở bài trước. Với mỗi dự đoán mà mô hình đưa ra, chúng ta đưa ra một giá trị mất mát do dự đoán sai là trị tuyệt đối của hiệu giữa giá trị dự đoán và giá trị thực trong bộ dữ liệu quan sát. Hay loss = $|y_{predicted} - y|$, trong đó $y_{predicted}$ là giá trị dự đoán của mô hình và $y$ là giá trị nhãn đúng trong bộ dữ liệu quan sát. Lấy một số điểm dự đoán ta tính được giá trị mất mát tại một số điểm trong bộ dữ liệu quan sát như sau:

| Gia trị thật $y$ | Giá trị dự báo $y_{predicted}$ | Giá trị mất mát do dự đoán sai |
| :--------------: |:------------------------------:| :-----------------------------:|
|        60        |              80                |                0               |
|        70        |              80                |                10              |
|        80        |              70                |                10              |

Như vậy, chúng ta có thể tính được giá trị mất mát do mô hình dự đoán tại tất cả các điểm trong bộ dữ liệu quan sát. Giá trị này có thể lớn, có thể nhỏ và khá rời rạc nên chúng ta không thể biết được đâu là dự đoán tốt, đâu là dự đoán không tốt, vì vậy để đánh giá mức độ tốt của mô hình dự đoán, chúng ta lấy trung bình giá trị mất mát của toàn bộ các điểm trên bộ dữ liệu quan sát. Điều này giúp ta xác định được một loss function trên toàn bộ bộ dữ liệu quan sát:

... = $\frac{1}{N} \Sigma_{i=1}^N |f_w(x^{(i)}) - y{(i)}|

![A](https://tek4.vn/public_files/644eab14-4616-4c28-bba7-35b04173c09d)

Ví dụ 2. Thay vì hàm trị tuyệt đối ở trên, bây giờ chúng ta có muốn phạt nặng nhiều lần hơn cho các dự đoán sai lớn, và để có thể tính được đạo hàm dễ dàng hơn, khi đó chúng ta có thể chọn hàm trung bình bình phương như đã nói ở bài trước:

...$(m_0,m_1)$ = $\frac{1}{2m} \Sigma_{i=1}^m (f(x_i,m_0,m_1) - y^2)^2

Nói tóm lại, loss function là các hàm phạt, đo lường mức độ dự đoán sai của mô hình trên dữ liệu quan sát hữu hạn, nó là trung bình lỗi dự đoán trên toàn bộ bộ dữ liệu và thường cần nhận giá trị thực, không âm (để đo cường độ và không làm triệt tiêu lỗi khi tính trung bình), ngoài ra nó nên có đạo hàm dễ xác định để tiện cho việc tối ưu hóa sử dụng các thuật toán dựa trên đạo hàm hoặc gradient sau này.

Các loss function này có thể được sử dụng trong cả quá trình huấn luyện lẫn quá trình đánh giá mô hình. Trong quá trình đánh giá mô hình nó được gọi là hàm đánh giá (evaluation function). Còn ngược lại khi chúng ta sử dụng các loss function này vào quá trình huấn luyện trên tập dữ liệu huấn luyện, chúng ta gọi nó là objective function hay hàm mục tiêu mà chúng ta cần tối ưu hóa.

Hầu hết mọi quá trình huấn luyện mô hình machine learning đều tập trung vào việc tối ưu hóa một hoặc một số hàm mục tiêu nào đó. Trong đa số các trường hợp nó là các hàm loss function hay hàm mất mát hoặc hàm phạt, nhưng cũng có những trường hợp nó là các hàm thưởng cần cực đại hóa. Tất cả chúng đều gọi chung là Objective Function hay hàm mục tiêu. Và hầu hết mọi bài toán machine learning đều quy về một bài toán tối ưu hàm mục tiêu. Các thuật toán machine learning đều quy về các thuật toán tối ưu và cũng như lĩnh vực tối ưu hóa trong toán học, cũng có rất nhiều các phương pháp để làm việc này. Chúng ta sẽ quay lại chi tiết một số hàm mất mát cũng như các phương pháp tối ưu ở các bài tiếp theo trong Series này.

Tham khảo [Loss Function (Hàm mất mát) và Objective Function (Hàm mục tiêu)](https://tek4.vn/khoa-hoc/machine-learning-co-ban/loss-function-ham-mat-mat-va-objective-function-ham-muc-tieu)
