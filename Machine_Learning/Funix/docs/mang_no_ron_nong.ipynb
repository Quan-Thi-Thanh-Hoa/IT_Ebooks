{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong bài học này, chúng ta sẽ bắt đầu tìm hiểu về cấu trúc Neural Network cơ bản - Shallow Neural Network (Mạng nơ-ron Nông) - một kiến trúc NN chỉ có một Hidden Layer (lớp ẩn) duy nhất. Tầm quan trọng của activation function (hàm kích hoạt) và việc khởi tạo các tham số sẽ được đề cập đến thông qua hàm kích hoạt sigmoid và phương pháp khởi tạo ngẫu nhiên. Ở phần cuối của bài học, các DL framework cơ bản sẽ được giới thiệu đến các bạn học viên như Tensorflow, Keras, Pytorch, Mxnet, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong hồi quy Logistic, chúng ta có:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Science%20%2FDSP303x_1.2_VN%2FContent_Image%2FDSP303x_1.2_L08_H01.png?alt=media&token=7697b7f9-d367-4291-ab76-ce7f942a6806)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong mạng nơ-ron một lớp, chúng ta có:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![B](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Science%20%2FDSP303x_1.2_VN%2FContent_Image%2FDSP303x_1.2_L08_H02.png?alt=media&token=705a06e2-7214-4688-8411-fce0ae4c4355)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X là vectơ đầu vào (X1, X2, X3), và Y là biến đầu ra (1x1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mạng nơ-ron là các stack xếp chồng của hồi quy Logistic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Neural Networks Overview](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/qg83v/neural-networks-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta lấy ví dụ mạng nơron có một lớp ẩn, tức là mạng nơron có các lớp đầu vào, lớp ẩn, lớp đầu ra. Lớp ẩn nghĩa là chúng ta không thể thấy lớp đó trong tập huấn luyện:\n",
    "\n",
    "- a0 = x (lớp đầu vào)\n",
    "- a1 biểu diễn kích hoạt của nơ-ron ẩn\n",
    "- a2 biểu diễn lớp đầu ra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta đang đề cập mạng nơron 2 lớp. Lớp đầu vào không được tính."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các phương trình cho lớp ẩn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Neural Network Representation](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/GyW9e/neural-network-representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![B](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Science%20%2FDSP303x_1.2_VN%2FContent_Image%2FDSP303x_1.2_L08_H03.png?alt=media&token=3261280e-5e46-4d40-872b-176afc2f85d6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong đó:\n",
    "\n",
    "- noOfHiddenNeurons (số nơron ẩn)  = 4\n",
    "- Nx = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape của các biến:\n",
    "\n",
    "- W1 là ma trận của lớp ẩn đầu tiên, có shape (noOfHiddenNeurons,nx)\n",
    "- b1 là ma trận của lớp ẩn đầu tiên, có shape (noOfHiddenNeurons,1)\n",
    "- z1 là kết quả của phương trình z1 = W1*X + b, có shape (noOfHiddenNeurons,1)\n",
    "- a1 là kết quả của phương trình a1 = sigmoid(z1), có shape (noOfHiddenNeurons,1)\n",
    "- W2 là ma trận của lớp ẩn thứ hai, có shape (1,noOfHiddenNeurons)\n",
    "- b2 là ma trận của lớp ẩn thứ hai, có shape (1,1)\n",
    "- z2 là kết quả của phương trình z2 = W2*a1 + b, có shape (1,1)\n",
    "- a2 là kết quả của phương trình a2 = sigmoid(z2), có shape (1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Computing a Neural Network's Output](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/tyAGh/computing-a-neural-networks-output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mã giả cho lan truyền xuôi trong mạng nơ-ron 2 lớp:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![M](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Science%20%2FDSP303x_1.2_VN%2FContent_Image%2FDSP303x_1.2_L08_H04.png?alt=media&token=f36ea0ac-2d4b-4696-888b-b60f906c0dd9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "với m là số cột.\n",
    "\n",
    "Trong ví dụ trước, chúng ta gọi X = A0. Do đó, bước trước có thể viết thành:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![M](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Science%20%2FDSP303x_1.2_VN%2FContent_Image%2FDSP303x_1.2_L08_H05.png?alt=media&token=7fab0e3a-b7b5-42d3-9e45-d20950ef58ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta đã sử dụng sigmoid nhưng trong một số trường hợp, các hàm khác sẽ tốt hơn. Sigmoid có thể dẫn tới vấn đề gradient mà các cập nhật khá thấp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biên độ hàm kích hoạt **Sigmoid  là [0,1]** trong đó **A = 1/(1 + np.exp(-z))** # trong đó: z là ma trận đầu vào."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biên độ hàm kích hoạt **Tanh là [-1,1]** (phiên bản thay đổi của hàm sigmoid). Trong NumPy, chúng ta có thể triển khai Tanh bằng một trong các phương thức sau: **A = (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))** hoặc **A = np.tanh(z)** với z là ma trận đầu vào. Như vậy, với các đơn vị ẩn, hàm kích hoạt Tanh thường hoạt động tốt hơn hàm kích hoạt Sigmoid vì trung bình đầu ra của nó gần 0 hơn và căn giữa dữ liệu tốt hơn cho lớp tiếp theo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm Sigmoid hoặc Tanh có hạn chế là nếu đầu vào quá nhỏ hoặc quá lớn thì slope sẽ gần 0, điều này gây ra vấn đề về gradient descent. Một trong những hàm kích hoạt phổ biến giúp xử lý gradient descent chậm là hàm **RELU. RELU = max(0,z).** Nếu z âm thì slope =1, nếu z dương thì slope vẫn tuyến tính."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đây là một số **quy tắc cơ bản để chọn hàm kích hoạt**; nếu phân loại giữa 0 và 1, sử dụng hàm đầu ra là sigmoid, còn lại hãy sử dụng RELU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm kích hoạt **Leaky RELU** khác với RELU ở chỗ nếu đầu vào âm thì slope sẽ rất nhỏ. Nó cũng hoạt động như RELU nhưng đa số mọi người thường sử dụng RELU nhiều hơn. **Leaky_RELU = max(0.01z,z)** với 0.01 có thể là tham số cho thuật toán."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong mạng nơ-ron, chúng ta cần đưa ra nhiều quyết định như:\n",
    "\n",
    "- Không có lớp ẩn\n",
    "- Không có nơron ở từng lớp ẩn\n",
    "- Tốc độ học (tham số quan trọng nhất)\n",
    "- Hàm kích hoạt\n",
    "- Những thứ khác"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Activation Functions](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4dDC1/activation-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu chúng ta loại hàm kích hoạt phi tuyến tính ra khỏi thuật toán, và thay vào đó hàm kích hoạt tuyến tính về bản chất đầu ra chỉ là giá trị của tuyến tính. Bất kể chúng ta thêm bao nhiêu lớp ẩn nào, kết quả cuối cùng cũng chỉ là tuyến tính. \n",
    "\n",
    "Chúng ta có thể sử dụng hàm kích hoạt tuyến tính ở một chỗ - trong lớp đầu ra nếu đầu ra là số thực (bài toán hồi quy). Nhưng kể cả trong trường hợp này, nếu giá trị đầu ra không âm, chúng ta có thể sử dụng RELU thay thế."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đạo hàm của hàm kích hoạt Sigmoid:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![V](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Science%20%2FDSP303x_1.2_VN%2FContent_Image%2FDSP303x_1.2_L08_H06.png?alt=media&token=c4cdaf9c-ed6e-4f9e-80f1-bb81985aa28f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đạo hàm của hàm kích hoạt Tanh:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![L](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Science%20%2FDSP303x_1.2_VN%2FContent_Image%2FDSP303x_1.2_L08_H07.png?alt=media&token=c2511da0-a3e6-4e00-adcb-e91362408f43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đạo hàm của hàm kích hoạt RELU:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![l](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Science%20%2FDSP303x_1.2_VN%2FContent_Image%2FDSP303x_1.2_L08_H08.png?alt=media&token=a8e2f24e-cb39-4bfc-9a9f-9e9a5876fb40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đạo hàm của hàm kích hoạt leaky RELU:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![o](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Science%20%2FDSP303x_1.2_VN%2FContent_Image%2FDSP303x_1.2_L08_H09.png?alt=media&token=51efa398-f406-4c05-ae07-3fa0bec624f6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở phần này, chúng ta sẽ tính lan truyền ngược đầy đủ của mạng nơ-ron.\n",
    "\n",
    "Các tham số mạng nơ-ron:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n[0] = Nx\n",
    "- n[1] = NoOfHiddenNeurons\n",
    "- n[2] = NoOfOutputNeurons = 1\n",
    "- W1 có shape là (n[1],n[0])\n",
    "- b1 có shape là (n[1],1)\n",
    "- W2 có shape là (n[2],n[1])\n",
    "- b2 có  shape là (n[2],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm chi phí **I =  I(W1, b1, W2, b2) = (1/m) * Sum(L(Y,A2))**\n",
    "\n",
    "Tính toán GD:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![M](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Science%20%2FDSP303x_1.2_VN%2FContent_Image%2FDSP303x_1.2_L08_H10.png?alt=media&token=a37a80c0-b43b-4392-ba9b-1ba5f5c2b10b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lan truyền xuôi:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![K](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Science%20%2FDSP303x_1.2_VN%2FContent_Image%2FDSP303x_1.2_L08_H11.png?alt=media&token=851faf47-f5cc-42fd-8fe4-b8e0a13977e1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lan truyền ngược:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![L](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Science%20%2FDSP303x_1.2_VN%2FContent_Image%2FDSP303x_1.2_L08_H12.png?alt=media&token=c409679a-9e2d-48ca-9fc8-1e81bb8119fb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![B](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Science%20%2FDSP303x_1.2_VN%2FContent_Image%2FDSP303x_1.2_L08_H13.png?alt=media&token=6e18e6c7-498b-45e5-acb9-d77431e23137)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Derivatives of Activation Functions](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/qcG1j/derivatives-of-activation-functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong mạng nơron, chúng ta cần khởi tạo trọng số ngẫu nhiên. \n",
    "\n",
    "Nếu chúng ta khởi tạo tất cả các trọng số bằng 0 trong mạng nơron, quá trình huấn luyện sẽ không hoạt động (khởi tạo độ chệch bằng 0 thì ổn) bởi vì:\n",
    "\n",
    "- Tất cả các đơn vị ẩn hoàn toàn giống hệt nhau (đối xứng) - tính toán chính xác các hàm hoàn toàn tương tự nhau.\n",
    "- Tất cả các đơn vị ẩn luôn cập nhật tương tự nhau ở từng lần lặp gradient descent.\n",
    "\n",
    "Để giải quyết, hãy khởi tạo W với các số nhỏ ngẫu nhiên:\n",
    "\n",
    "- W1 = np.random.randn((2,2)) * 0.01 (0.01 để khiến W nhỏ)\n",
    "- b1 = np.zeros((2,1)) (b = 0 cũng ổn, nó sẽ không đưa chúng ta vào vấn đề đối xứng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Random Initialization](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/XtFPI/random-initialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong bài này, chúng ta đã đi qua các phần kiến thức sau:\n",
    "\n",
    "- Xây dựng một shallow neural network thông qua các quá trình forward và backward.\n",
    "- Activation function và vai trò của nó trong neural network.\n",
    "- Đạo hàm của hàm sigmoid.\n",
    "- Khởi tạo ngẫu nhiên, tại sao chúng ta cần sử dụng khởi tạo ngẫu nhiên."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ad7f758c7f2ebdda2cc4f553f1363ed8fbfe288e0aece9aaf2ec73dff90b611"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
