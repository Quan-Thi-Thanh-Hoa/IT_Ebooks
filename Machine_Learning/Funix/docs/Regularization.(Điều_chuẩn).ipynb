{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong bài học này, chúng ta sẽ tìm hiểu về Regularization (Điều chuẩn) - một trong những kỹ thuật chủ đạo để giảm bớt Overfitting. Ngoài ra, các kỹ thuật Regularization khác cũng sẽ được giới thiệu đến các bạn học viên. Ở phần sau của bài học, chúng ta sẽ đối mặt với 2 vấn đề rất lớn về tính hội tụ trong mạng DNN là Gradient Vanishing và Exploding (Tiêu biến và bùng nổ Gradient), từ đó đi sâu vào các phương pháp cải thiện 2 vấn đề ở trên như là Normalize Input (Chuẩn hóa đầu vào), khởi tạo tham số đầu vào hợp lý theo chuẩn hóa đầu vào (một mức độ sâu hơn về khởi tạo tham số mà chúng ta đã được học)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Train / Dev / Test sets](https://www.coursera.org/learn/deep-neural-network/lecture/cxG1s/train-dev-test-sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Bias / Variance](https://www.coursera.org/learn/deep-neural-network/lecture/ZhclI/bias-variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu thuật toán có độ chệch cao:\n",
    "\n",
    "- Thử khiến mạng nơron lớn hơn (kích thước các đơn vị ẩn, số lớp).\n",
    "- Thử mô hình khác phù hợp với dữ liệu.\n",
    "- Thử chạy nó lâu hơn.\n",
    "- Các thuật toán tối ưu (nâng cao) khác.\n",
    "\n",
    "Nếu thuật toán có phương sai cao:\n",
    "\n",
    "- Thêm nhiều dữ liệu hơn.\n",
    "- Thử điều chuẩn hóa.\n",
    "- Thử mô hình khác phù hợp với dữ liệu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Basic Recipe for Machine Learning](https://www.coursera.org/learn/deep-neural-network/lecture/ZBkx4/basic-recipe-for-machine-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Điều chuẩn trong DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong phần này, chúng ta sẽ học 2 phương pháp regularization (điều chuẩn) chính: L1 - L2 regularization và dropout. Ngoài ra, các phương pháp điều chuẩn khác cũng sẽ được giới thiệu đến cho các bạn học viên như data augmentation và early stopping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thêm điều chuẩn vào mạng nơron sẽ giúp giảm phương sai (overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chuẩn ma trận $L1: ||W|| = Sum(|w[i,j]|)$  (tổng các giá trị tuyệt đối của w)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chuẩn ma trận L2 (chuẩn Frobenius): $||W||^2 = Sum(|w[i,j]|^2)$  (tổng tất cả các w bình phương). Cũng có thể tính như $||W||^2 = W.T * W$ nếu $W$ là vectơ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Điều chuẩn cho hồi quy logistic:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hàm chi phí chuẩn mà chúng ta cần tối thiểu hóa là: $J(w,b) = (1/m) * Sum(L(y(i),y'(i))).$\n",
    "- Phiên bản điều chuẩn L2: $J(w,b) = (1/m) * Sum(L(y(i),y'(i))) + (lambda/2m) * Sum(|w[i]|^2).$\n",
    "- Phiên bản điều chuẩn L1: $J(w,b) = (1/m) * Sum(L(y(i),y'(i))) + (lambda/2m) * Sum(|w[i]|).$\n",
    "- Phiên bản điều chuẩn L1 khiến nhiều giá trị thành 0, điều này khiến kích thước mô hình nhỏ hơn.\n",
    "- Điều chuẩn L2 hiện được dùng khá thường xuyên.\n",
    "- lambda ở đây là tham số điều chuẩn (siêu tham số).\n",
    "\n",
    "Điều chuẩn cho mạng nơron:\n",
    "\n",
    "- Hàm chi phí chuẩn mà chúng ta cần tối thiểu hóa: $J(W1,b1...,WL,bL) = (1/m) * Sum(L(y(i),y'(i))).$\n",
    "- Phiên bản điều chuẩn L2: $J(w,b) = (1/m) * Sum(L(y(i),y'(i))) + (lambda/2m) * Sum((||W[l]||^2).$\n",
    "- Chúng ta xếp chồng ma trận thành một vectơ $(mn,1)$ rồi áp dụng $sqrt(w1^2 + w2^2.....).$\n",
    "- Thực hiện lan truyền ngược (cách cũ): $dw[l] = (from back propagation).$\n",
    "- Cách mới: $dw[l] = (from back propagation) + lambda/m * w[l].$\n",
    "\n",
    "Vậy hãy thay nó vào bước cập nhật trọng số:\n",
    "\n",
    "![](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Science%20%2FDSP303x_1.2_VN%2FContent_Image%2FDSP303x_1.2_L10_H01.png?alt=media&token=eeef0471-55c0-4c43-bc57-4fb77637e415)\n",
    "\n",
    "Thực tế, điều này phạt các trọng số lớn và giới hạn hiệu quả tự do trong mô hình.\n",
    "\n",
    "Phần tử mới $(1 - (learning_rate*lambda)/m) * w[l]$ khiến trọng số suy giảm tỷ lệ với kích thước của nó.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Regularization](https://www.coursera.org/learn/deep-neural-network/lecture/Srsrc/regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dưới đây là một số trực quan:\n",
    "\n",
    "Trực quan 1:\n",
    "\n",
    "- Nếu lambda quá lớn - nhiều w sẽ gần bằng 0, điều này giúp mạng nơron đơn giản hơn (các bạn có thể coi nó hoạt động gần giống hồi quy logistic).\n",
    "- Nếu lambda đủ tốt, nó sẽ giảm một vài trọng số khiến mạng nơron quá khớp.\n",
    "\n",
    "Trực quan 2 (với hàm kích hoạt Tanh): \n",
    "\n",
    "- Nếu lambda quá lớn, w sẽ nhỏ (gần bằng 0) - chúng ta sẽ dùng các phần tuyến  tính của hàm kích hoạt tanh nên hãy đi từ kích hoạt phi tuyến tính tới gần tuyến tính, làm cho mạng nơron thành phân loại gần như tuyến tính.\n",
    "- Nếu lambda đủ tốt, nó sẽ chỉ khiến một số hàm tanh gần như tuyến tính, giúp ngăn overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mẹo thực hiện: Nếu triển khai gradient descent, một trong các bước gỡ lỗi gradient descent là vẽ biểu đồ hàm chi phí J thành hàm của số lần lặp của gradient descent và chúng ta sẽ muốn hàm chi phí J đó giảm đều sau mỗi lần nâng gradient descent với điều chuẩn. Nếu vẽ biểu đồ khái niệm J cũ (không điều chuẩn) thì sẽ không thấy nó giảm đều nữa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Why Regularization Reduces Overfitting?](https://www.coursera.org/learn/deep-neural-network/lecture/T6OJj/why-regularization-reduces-overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở đa số trường hợp, Andrew Ng dùng điều chuẩn L2. Điều chuẩn dropout loại bỏ một vài nơron/trọng số ở từng lần lặp theo xác suất. Kỹ thuật triển khai dropout thường gặp nhất là \"Inverted dropout\".\n",
    "\n",
    "Code mẫu cho dropout: \n",
    "\n",
    "![](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Science%20%2FDSP303x_1.2_VN%2FContent_Image%2FDSP303x_1.2_L10_H02.png?alt=media&token=22ec59c5-93e3-4b79-9d21-a1137dc52f7b)\n",
    "\n",
    "[Dropout Regularization](https://www.coursera.org/learn/deep-neural-network/lecture/eM33A/dropout-regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta biết rằng dropout sẽ loại ngẫu nhiên các đơn vị ra khỏi mạng nơron, sẽ như vậy ở mỗi lần lặp, chúng ta làm việc với mạng nơron nhỏ hơn, vậy nên sử dụng mạng nơron nhỏ hơn giống như có tác dụng điều chuẩn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Không thể dựa vào một đặc trưng nên chúng ta cần trải rộng các trọng số.\n",
    "- Không thể chỉ ra dropout có tác động tương tự với điều chuẩn L2.\n",
    "- Dropout có thể có keep_prob khác ở mỗi lớp.\n",
    "- Dropout lớp đầu vào phải gần 1 (hoặc 1 - không dropout) vì chúng ta không muốn loại nhiều đặc trưng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu lo lắng một số lớp quá khớp hơn so với các lớp khác, chúng ta có thể thiết lập keep_prob thấp hơn cho một vài lớp. Nhược điểm là điều này cho nhiều siêu tham số để tìm kiếm bằng kiểm định chéo. Cách khác là có các lớp mà chúng ta áp dụng dropout và một số lớp thì không, sau đó chỉ có một siêu tham số là keep_prob cho các lớp mà chúng ta áp dụng dropout. Nhiều nhà nghiên cứu đang sử dụng dropout với Thị giác máy tính vì chúng có kích thước đầu vào lớn và hầu như không bao giờ có đủ dữ liệu, do đó overfitting là vấn đề thường thấy. Dropout là kỹ thuật điều chuẩn giúp ngăn chặn vấn đề này.\n",
    "\n",
    "Nhược điểm của dropout là hàm chi phí J không được xác định tốt và khó gỡ lỗi (vẽ biểu đồ J theo lần lặp). Để xử lý, chúng ta cần tắt dropout, đặt tất cả the keep_probs thành 1 rồi chạy code và kiểm tra xem nó đã giảm I đều rồi bật lại dropout.\n",
    "\n",
    "[Understanding Dropout](https://www.coursera.org/learn/deep-neural-network/lecture/YaGbR/understanding-dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một số phương pháp điều chuẩn khác:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tăng cường dữ liệu (data augmentation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ví dụ dữ liệu trong Thị giác máy tính: Có thể lật ảnh theo chiều ngang, điều này cho nhiều thực thể dữ liệu hơn. Cũng có thể áp dụng xoay và vị trí ngẫu nhiên cho ảnh để lấy thêm dữ liệu.\n",
    "- Ví dụ: trong OCR, chúng ta có thể đặt các phép xoay và biến dạng ngẫu nhiên vào chữ cái/số.\n",
    "- Dữ liệu mới thu được nhờ kỹ thuật này không tốt như dữ liệu độc lập thực tế nhưng vẫn dùng như kỹ thuật điều chuẩn được."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Early stopping (Ngừng sớm)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Science%20%2FDSP303x_1.2_VN%2FContent_Image%2FDSP303x_1.2_L10_H03.png?alt=media&token=5beff80e-b41d-41c7-9c84-7a7aece27821)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trong kỹ thuật này, chúng ta sẽ vẽ biểu đồ tập huấn luyện và tập dev, cùng thiết lập chi phí cho mỗi lần lặp. Ở một số lần lặp, chi phí dev sẽ ngừng giảm và bắt đầu tăng.\n",
    "* Chúng ta sẽ chọn điểm mà lỗi huấn luyện và lỗi dev tốt nhất (chi phí huấn luyện và dev thấp nhất), lấy các tham số đó làm tham số tốt nhất."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Other Regularization Methods](https://www.coursera.org/learn/deep-neural-network/lecture/Pa53F/other-regularization-methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thiết lập các vấn đề tối ưu Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta sẽ cùng nhau tìm hiểu sâu hơn về vấn đề vanishing/exploding gradients và cách khắc phục vấn đề này thông qua 2 phương pháp chính: Normalizing input và khởi tạo trọng số nâng cao.\n",
    "\n",
    "Nếu chuẩn hóa đầu vào, điều này sẽ giúp tăng tốc độ quá trình huấn luyện khá nhiều.\n",
    "\n",
    "Chuẩn hóa gồm các bước sau:\n",
    "\n",
    "1. Tính trung bình của tập huấn luyện: $mean = (1/m) * sum(x(i))$\n",
    "2. Trừ từng đầu vào cho trung bình: $X = X - mean$ (Điều này khiến các đầu vào căn giữa ở 0)\n",
    "3. Tính phương sai của tập huấn luyện: $variance = (1/m) * sum(x(i)^2)$\n",
    "4. Chuẩn hóa phương sai: $X /= variance$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các bước trên áp dụng cho tập huấn luyện, dev và tập kiểm tra (nhưng dùng trung bình và phương sai của tập huấn luyện).\n",
    "\n",
    "**Tại sao cần chuẩn hóa?**\n",
    "\n",
    "- Nếu không chuẩn hóa đầu vào, hàm chi phí sẽ sâu và hình dạng không đồng nhất và tốn nhiều thời gian tối ưu.\n",
    "- Ngược lại, nếu chuẩn hóa thì sẽ không xảy ra những vấn đề trên. Hình dạng hàm chi phí sẽ đồng nhất (trông đối xứng hơn như đường tròn trong ví dụ 2D) và chúng ta có thể sử dụng tỷ lệ học alpha lớn hơn - tối ưu hóa cũng sẽ nhanh hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Normalizing Inputs](https://www.coursera.org/learn/deep-neural-network/lecture/lXv6U/normalizing-inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanishing/Exploding gradient xảy ra khi đạo hàm trở nên rất nhỏ hoặc rất lớn.  \n",
    "\n",
    "Giả sử chúng ta có mạng nơron sâu với L lớp, tất cả các hàm kích hoạt tuyến tính và mỗi b = 0.\n",
    "\n",
    "Như vậy: $Y' = W[L]W[L-1].....W[2]W[1]X$\n",
    "\n",
    "Nếu có 2 đơn vị ẩn ở mỗi lớp và $x1 = x2 = 1$ thì được kết quả:\n",
    "\n",
    "![](https://firebasestorage.googleapis.com/v0/b/funix-way.appspot.com/o/xSeries%2FData%20Science%20%2FDSP303x_1.2_VN%2FContent_Image%2FDSP303x_1.2_L10_H04.png?alt=media&token=2e690159-183e-42d6-8c5e-0ee340dd8c7f)\n",
    "\n",
    "Ví dụ cuối giải thích rằng hàm kích hoạt (tương tự với đạo hàm) sẽ tăng/giảm theo cấp số nhân như hàm số lớp.\n",
    "\n",
    "nếu W > I (ma trận đơn vị) thì gradient sẽ rất lớn (exploding)\n",
    "\n",
    "nếu W < I (ma trận đơn vị) thì kích hoạt và gradient sẽ rất nhỏ (vanishing)\n",
    "\n",
    "Microsoft đã huấn luyện 1 mô hình152 lớp (ResNet)!, con số này rất lớn. Với mạng nơron sâu như vậy, nếu kích hoạt hoặc gradient tăng hoặc giảm theo cấp số nhân như hàm của L thì các giá trị đó sẽ vô cùng lớn hoặc vô cùng nhỏ. Điều này khiến huấn luyện khó hơn, đặc biệt là nếu gradient nhỏ hơn rất nhiều so với L. Do đó, gradient descent sẽ thực hiện các bước rất nhỏ và tốn nhiều thời gian để học mọi thứ.\n",
    "\n",
    "[Vanishing / Exploding Gradients](https://www.coursera.org/learn/deep-neural-network/lecture/C9iQO/vanishing-exploding-gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có một giải pháp riêng không hoàn toàn giải được bài toán này nhưng khá là hữu ích lựa chọn cẩn thận cách khởi tạo trọng số.\n",
    "\n",
    "Trong mạng nơron đơn lẻ (mô hình Perceptron): $Z = w1x1 + w2x2 + ... + wnxn.$ Nếu $n_x$ lớn, cần $W$ nhỏ hơn để không bùng nổ chi phí. Như vậy chúng ta cần phương sai bằng $1/n_x$ là biên độ của $W$.\n",
    "\n",
    "Giả sử chúng ta khởi tạo W như thể này (nên sử dụng với kích hoạt $tanh$):\n",
    "\n",
    "- $np.random.rand(shape) * np.sqrt(1/n[l-1])$\n",
    "\n",
    "hoặc theo công thức của Bengio:\n",
    "\n",
    "- $np.random.rand(shape) * np.sqrt(2/(n[l-1] + n[l]))$\n",
    "\n",
    "Thiết lập phần khởi tạo trong căn bậc hai của $2/n[l-1]$ cho $ReLU$:\n",
    "\n",
    "- $np.random.rand(shape) * np.sqrt(2/n[l-1])$\n",
    "\n",
    "Công thức 1 hoặc 2 ở tử số có thể là siêu tham số cần chỉnh.\n",
    "\n",
    "Đây là cách tốt nhất của giải pháp cho Vanishing/Exploding gradient (ReLU + Khởi tạo trọng số với phương sai), giúp gradient không biến mất/nổ quá nhanh.\n",
    "\n",
    "Khởi tạo được giới thiệu trong video sẽ là $\"He Initialization/Xavier Initialization\"$, được đưa ra năm 2015.\n",
    "\n",
    "[Weight Initialization for Deep Networks](https://www.coursera.org/learn/deep-neural-network/lecture/RwqYe/weight-initialization-for-deep-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tổng quan về tối ưu trong DL model\n",
    "- Các phương án cơ bản để giảm underfitting/overfitting\n",
    "- L1 - L2 regularization\n",
    "- Dropout\n",
    "- Cơ bản về data augmentation, early stopping\n",
    "- Input normalization\n",
    "- Vanishing/Exploding Gradient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ad7f758c7f2ebdda2cc4f553f1363ed8fbfe288e0aece9aaf2ec73dff90b611"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
